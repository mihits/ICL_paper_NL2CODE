02/10/2024 15:38:41 - INFO - __main__ - Namespace(batch_size=16, data_dir='data/', dataset=None, do_sanity_check=False, do_tensorize=True, fp16=False, gpt2='gpt2', k=16384, local_rank=-1, log_dir='logs', log_period=2, lr=0.01, method='direct', n_gpu=1, n_prefix_tokens=10, n_process=10, no_masking=False, num_training_steps=5, optimization='adamw', out_dir='checkpoints/gpt2/code_dataset-train/prefix={10}-{direct}-lr={1e-2}-initByVocab', prefix_embed_file=None, save_period=1000, seed=100, split='train', task='code_dataset', tensorize_dir='tensorized', test_k=4, train_seed=1, use_demonstrations=False, use_random_english_words=False, warmup_steps=0, weight_decay=0.0)
02/10/2024 15:38:41 - INFO - __main__ - batch_size=16	max_length=1024	max_length_per_example=1024
02/10/2024 15:38:41 - INFO - __main__ - [Train] humaneval	284
02/10/2024 15:38:41 - INFO - __main__ - direct on None (1 train)
02/10/2024 15:38:57 - INFO - __main__ - tensorized/code_dataset_direct_k=284_seed=100_length=10-1024-rank=%d.pkl
02/10/2024 15:39:06 - INFO - __main__ - Preprocessing done for i=0
02/10/2024 15:39:06 - INFO - __main__ - Finish saving preprocessed data ...
02/10/2024 15:39:06 - INFO - __main__ - tensorized/code_dataset_direct_k=284_seed=100_length=10-1024-rank=%d.pkl
02/10/2024 15:39:06 - INFO - __main__ - Checking the first example...
Input:
<humaneval-0><humaneval-1><humaneval-2><humaneval-3><humaneval-4><humaneval-5><humaneval-6><humaneval-7><humaneval-8><humaneval-9>Write a Python function `rescale_to_unit(numbers: List[float]) -> List[float]` to solve the following problem:
Given list of numbers (of at least two elements), apply a linear transform to that list,
such that the smallest number will become 0 and the largest will become 1
>>> rescale_to_unit([1.0, 2.0, 3.0, 4.0, 5.0])
[0.0, 0.25, 0.5, 0.75, 1.0]
Output:
     min_number = min(numbers)
    max_number = max(numbers)
    return [(x - min_number) / (max_number - min_number) for x in numbers]

02/10/2024 15:39:06 - INFO - __main__ - checkpoints/gpt2/code_dataset-train/prefix={10}-{direct}-lr={1e-2}-initByVocab
02/10/2024 15:39:06 - INFO - __main__ - Setting up for local_rank=-1, world_size=1
02/10/2024 15:39:12 - INFO - __main__ - torch.Size([284, 1024])
02/10/2024 15:39:12 - INFO - __main__ - Training 1 parameters on 284 examples for 5 steps using 0 GPUs
